# Simple Wikipedia Crawler

A simple crawler for Wikipedia pages. Given a Wikipedia page title it finds every internal link and keeps crawling until reaching the given depth. If no depth is given it will crawl until stopped. When the crawler stops it creates a .txt for every page crawled with a list of every link found in that page.

# License

MIT licensed. See the [LICENSE](https://github.com/Alien1993/Simple-Wikipedia-Crawler/blob/master/LICENSE) file for full details.

# Credits

I want to thank
* [goldsmith](https://github.com/goldsmith) for making the [Wikipedia](https://github.com/goldsmith/Wikipedia) module
* the [Wikimedia Foundation](https://wikimediafoundation.org/wiki/Home) for making this possible
